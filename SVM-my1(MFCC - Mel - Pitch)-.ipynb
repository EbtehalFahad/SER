{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e8872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile, librosa, pickle, glob, os, parselmouth, pywt, statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from parselmouth.praat import call\n",
    "from scipy.io import wavfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "# all emotions on RAVDESS dataset\n",
    "allemotion = {\n",
    "    \"01\": \"angry\",\n",
    "    \"02\": \"happy\",\n",
    "    \"03\": \"neutral\",\n",
    "    \"04\": \"sad\"\n",
    "}\n",
    "# Emotions to observe\n",
    "observed_emotions={\n",
    "    \"angry\",\n",
    "    \"sad\",\n",
    "    \"neutral\",\n",
    "    \"happy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88029adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features (mfcc, chroma, mel, contrast, tonnetz) from a sound file\n",
    "def extract_features(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        # read the sound\n",
    "        sound = parselmouth.Sound(file_name)\n",
    "        #duration\n",
    "        duration = sound.get_end_time() # duration\n",
    "        \n",
    "        # extract features from librosa (MFCC, Mel)\n",
    "        \n",
    "        #if chroma:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        #if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "        #if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "        #result = np.hstack((result, mel))\n",
    "        \n",
    "        \n",
    "        # extract features from Praat (pitch, intensity, formant, HNR, LTAS, Jitter, Shimmer)\n",
    "        \n",
    "        #pitch\n",
    "        pitch = sound.to_pitch()\n",
    "        #pitch = call(sound, \"To Pitch\", 0.0, 75.0, 600.0) #create a praat pitch object\n",
    "        mean_F0 = call(pitch, \"Get mean\", 0, duration, \"Hertz\") # get mean pitch\n",
    "        stdev_F0 = call(pitch, \"Get standard deviation\", 0 ,duration, \"Hertz\") # get standard deviation\n",
    "        result = np.hstack((result, mean_F0))\n",
    "        result = np.hstack((result, stdev_F0))\n",
    "\n",
    "        #intensity\n",
    "        intensity = sound.to_intensity()\n",
    "        mean_intensity = call(intensity, \"Get mean\", 0, duration) # get mean intensity\n",
    "        stdev_intensity = call(intensity, \"Get standard deviation\", 0 ,duration) # get standard deviation intensity\n",
    "        result = np.hstack((result, mean_intensity))\n",
    "        result = np.hstack((result, stdev_intensity))\n",
    "        \n",
    "        #Formant\n",
    "        formant= sound.to_formant_burg()\n",
    "        f1_mean = call(formant, \"Get mean\", 1, 0, duration, \"Hertz\") # get mean 1st Formant\n",
    "        f2_mean = call(formant, \"Get mean\", 2, 0, duration, \"Hertz\") # get mean 2st Formant\n",
    "        f3_mean = call(formant, \"Get mean\", 3, 0, duration, \"Hertz\") # get mean 3st Formant\n",
    "        f1_stdev = call(formant, \"Get standard deviation\", 1, 0, duration, \"Hertz\") # get standard deviation 1st Formant\n",
    "        f2_stdev = call(formant, \"Get standard deviation\", 2, 0, duration, \"Hertz\") # get standard deviation 2st Formant\n",
    "        f3_stdev = call(formant, \"Get standard deviation\", 3, 0, duration, \"Hertz\") # get standard deviation 3st Formant\n",
    "        result = np.hstack((result, f1_mean))\n",
    "        result = np.hstack((result, f2_mean))\n",
    "        result = np.hstack((result, f3_mean))\n",
    "        #result = np.hstack((result, f1_stdev))\n",
    "        #result = np.hstack((result, f2_stdev))\n",
    "        #result = np.hstack((result, f3_stdev))\n",
    "        \n",
    "        \"\"\"\n",
    "        #LTAS\n",
    "        spectrum = sound.to_spectrum()\n",
    "        ltas = call(spectrum, \"To Ltas (1-to-1)\")\n",
    "        ltas_mean = call(ltas, \"Get mean\", 0, 0, \"dB\") # get mean intensity\n",
    "        result = np.hstack((result, ltas_mean))\"\"\"\n",
    "        \n",
    "        #HNR\n",
    "        harmonicity = sound.to_harmonicity() \n",
    "        hnr = call(harmonicity, \"Get mean\", 0, duration)\n",
    "        result = np.hstack((result, hnr))\n",
    "        \n",
    "        \n",
    "        #Jitter\n",
    "        pointProcess = call(sound, \"To PointProcess (periodic, cc)\", 75.0, 600.0)\n",
    "        \n",
    "        #local 1\n",
    "        localJitter = call(pointProcess, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "        result = np.hstack((result, localJitter))\n",
    "        #local-absolute 3\n",
    "        #localabsoluteJitter = call(pointProcess, \"Get jitter (local, absolute)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "        #result = np.hstack((result, localabsoluteJitter))\n",
    "        #rap 2\n",
    "        rapJitter = call(pointProcess, \"Get jitter (rap)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "        result = np.hstack((result, rapJitter))\n",
    "        \n",
    "        \n",
    "        #Shimmer\n",
    "        # localShimmer\n",
    "        localShimmer =  call([sound, pointProcess], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "        result = np.hstack((result, localShimmer))\n",
    "        \"\"\"#localdbShimmer\n",
    "        localdbShimmer = call([sound, pointProcess], \"Get shimmer (local_dB)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "        result = np.hstack((result, localdbShimmer))\"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # extract features from pywt (wavelet features)\n",
    "        # wavelet features\n",
    "        sample_rate, data = wavfile.read(file_name)# Reading the audio file\n",
    "        t = np.arange(len(data)) / float(sample_rate);  # Retrieving Time\n",
    "\n",
    "        data = data/max(data);  # Normalize Audio Data    \n",
    "\n",
    "        #cA, cD = pywt.dwt(data, \"db4\", \"per\") #DWT\n",
    "\n",
    "        coeffs = pywt.wavedec(data, 'db4', mode='sym', level=4);  # DWT\n",
    "        \n",
    "        cA4, cD4, cD3, cD2, cD1 = coeffs\n",
    "        result = np.hstack((result, statistics.mean(cD1)))\n",
    "        result = np.hstack((result, statistics.mean(cD2)))\n",
    "        result = np.hstack((result, statistics.mean(cD3)))\n",
    "        result = np.hstack((result, statistics.mean(cD4)))\n",
    "        result = np.hstack((result, statistics.mean(cA4)))\n",
    "        \n",
    "        result = np.hstack((result, statistics.mean(cD)))\n",
    "        result = np.hstack((result, statistics.mean(cA)))\"\"\"\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03b2de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_data(test_size):\n",
    "    x, y = [], []\n",
    "    try :\n",
    "        for file in glob.glob(\"Data/NActor_*/*.wav\"):\n",
    "            # get the base name of the audio file\n",
    "            basename = os.path.basename(file)\n",
    "            # get the emotion label\n",
    "            emotion = allemotion[basename.split(\" \")[0].split(\"-\")[3]] \n",
    "            # we allow only observed_emotions we set for both gender\n",
    "            if emotion not in observed_emotions:\n",
    "                continue\n",
    "            # extract speech features\n",
    "            features = extract_features(file, mfcc=True, chroma=True, mel=True)\n",
    "            # add to data\n",
    "            x.append(features)\n",
    "            y.append(emotion)\n",
    "    except :\n",
    "         pass\n",
    "    # split the data to training and testing and return it\n",
    "    return train_test_split(np.array(x), np.array(y), test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc04a785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both gender 80/20\n",
      "[+] Number of training samples: 515\n",
      "[+] Number of testing samples: 129\n",
      "[+] Number of features: 51\n",
      "Accuracy linear kernel: 57.36%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.57      0.79      0.67        34\n",
      "       happy       0.53      0.51      0.52        37\n",
      "     neutral       0.60      0.54      0.57        39\n",
      "         sad       0.64      0.37      0.47        19\n",
      "\n",
      "    accuracy                           0.57       129\n",
      "   macro avg       0.58      0.55      0.56       129\n",
      "weighted avg       0.58      0.57      0.57       129\n",
      "\n",
      "[[27  3  2  2]\n",
      " [11 19  7  0]\n",
      " [ 6 10 21  2]\n",
      " [ 3  4  5  7]]\n"
     ]
    }
   ],
   "source": [
    "#loading_data\n",
    "print(\"Both gender 80/20\")\n",
    "x_train,x_test,y_train,y_test = load_data(test_size = 0.20)\n",
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(x_train, y_train)\n",
    "\n",
    "# print some details\n",
    "# number of samples in training data\n",
    "print(\"[+] Number of training samples:\", x_train.shape[0])\n",
    "# number of samples in testing data\n",
    "print(\"[+] Number of testing samples:\", x_test.shape[0])\n",
    "# number of features used\n",
    "# this is a vector of features extracted \n",
    "# using utils.extract_features() method\n",
    "print(\"[+] Number of features:\", x_train.shape[1])\n",
    "#________________________________________________________________________\n",
    "#linear kernel (best result is linear kernel)\n",
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(x_train, y_train) \n",
    "svm_predictions = svm_model_linear.predict(x_test) \n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(y_true=y_test,y_pred=svm_predictions)\n",
    "print(\"Accuracy linear kernel: {:.2f}%\".format(accuracy*100))\n",
    "print(classification_report(y_test,svm_predictions))\n",
    "print(confusion_matrix(y_test, svm_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda64e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
